{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from kerastuner import Hyperband, HyperModel\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/winequality/winequality-red.csv', delimiter=';')\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
    "val_test_dataset = dataset.drop(train_dataset.index)\n",
    "val_dataset = val_test_dataset.sample(frac=0.5, random_state=0)\n",
    "test_dataset = val_test_dataset.drop(val_dataset.index)\n",
    "\n",
    "train_features = train_dataset.copy()\n",
    "val_features = val_dataset.copy()\n",
    "test_features = test_dataset.copy()\n",
    "\n",
    "train_labels = train_features.pop('quality')\n",
    "val_labels = val_features.pop('quality')\n",
    "test_labels = test_features.pop('quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value = 1\n",
    "max_value = 8\n",
    "step_size = 1\n",
    "\n",
    "class DeepHyperModel(HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = tf.keras.Sequential([\n",
    "            layers.experimental.preprocessing.Normalization(),\n",
    "            layers.Dense(units=hp.Int(\n",
    "                'units_1',\n",
    "                min_value=min_value,\n",
    "                max_value=max_value,\n",
    "                step=step_size\n",
    "            ), activation='relu'),\n",
    "            layers.Dense(units=hp.Int(\n",
    "                'units_2',\n",
    "                min_value=min_value,\n",
    "                max_value=max_value,\n",
    "                step=step_size\n",
    "            ), activation='relu'),\n",
    "            layers.Dense(units=hp.Int(\n",
    "                'units_3',\n",
    "                min_value=min_value,\n",
    "                max_value=max_value,\n",
    "                step=step_size\n",
    "            ), activation='relu'),\n",
    "            layers.Dense(units=hp.Int(\n",
    "                'units_4',\n",
    "                min_value=min_value,\n",
    "                max_value=max_value,\n",
    "                step=step_size\n",
    "            ), activation='relu'),\n",
    "            layers.Dense(units=hp.Int(\n",
    "                'units_5',\n",
    "                min_value=min_value,\n",
    "                max_value=max_value,\n",
    "                step=step_size\n",
    "            ), activation='relu'),\n",
    "            layers.Dense(units=hp.Int(\n",
    "                'units_6',\n",
    "                min_value=min_value,\n",
    "                max_value=max_value,\n",
    "                step=step_size\n",
    "            ), activation='relu'),\n",
    "            layers.Dense(units=hp.Int(\n",
    "                'units_7',\n",
    "                min_value=min_value,\n",
    "                max_value=max_value,\n",
    "                step=step_size\n",
    "            ), activation='relu'),\n",
    "            layers.Dense(units=hp.Int(\n",
    "                'units_8',\n",
    "                min_value=min_value,\n",
    "                max_value=max_value,\n",
    "                step=step_size\n",
    "            ), activation='relu'),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Float(\n",
    "                'learning_rate',\n",
    "                min_value=1e-6,\n",
    "                max_value=0.01,\n",
    "                sampling='LOG'\n",
    "            )),\n",
    "            loss='mean_squared_error'\n",
    "        )\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project hyperband/deep_net/oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from hyperband/deep_net/tuner0.json\n",
      "Search space summary\n",
      "Default search space size: 9\n",
      "units_1 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 8, 'step': 1, 'sampling': None}\n",
      "units_2 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 8, 'step': 1, 'sampling': None}\n",
      "units_3 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 8, 'step': 1, 'sampling': None}\n",
      "units_4 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 8, 'step': 1, 'sampling': None}\n",
      "units_5 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 8, 'step': 1, 'sampling': None}\n",
      "units_6 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 8, 'step': 1, 'sampling': None}\n",
      "units_7 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 8, 'step': 1, 'sampling': None}\n",
      "units_8 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 8, 'step': 1, 'sampling': None}\n",
      "learning_rate (Float)\n",
      "{'default': 1e-06, 'conditions': [], 'min_value': 1e-06, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
     ]
    }
   ],
   "source": [
    "HYPERBAND_MAX_EPOCHS = 200\n",
    "EXECUTION_PER_TRIAL = 3\n",
    "\n",
    "hypermodel = DeepHyperModel()\n",
    "\n",
    "tuner = Hyperband(\n",
    "    hypermodel,\n",
    "    max_epochs=HYPERBAND_MAX_EPOCHS,\n",
    "    objective='val_loss',\n",
    "    seed=42,\n",
    "    executions_per_trial=EXECUTION_PER_TRIAL,\n",
    "    directory='hyperband',\n",
    "    project_name='deep_net'\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(train_features, train_labels, validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 31.1665 - val_loss: 27.0144\n",
      "Epoch 2/200\n",
      "40/40 [==============================] - 0s 952us/step - loss: 17.1926 - val_loss: 7.4353\n",
      "Epoch 3/200\n",
      "40/40 [==============================] - 0s 953us/step - loss: 5.8021 - val_loss: 5.1508\n",
      "Epoch 4/200\n",
      "40/40 [==============================] - 0s 976us/step - loss: 4.3178 - val_loss: 3.7366\n",
      "Epoch 5/200\n",
      "40/40 [==============================] - 0s 941us/step - loss: 3.0083 - val_loss: 2.5425\n",
      "Epoch 6/200\n",
      "40/40 [==============================] - 0s 938us/step - loss: 1.9353 - val_loss: 1.5872\n",
      "Epoch 7/200\n",
      "40/40 [==============================] - 0s 975us/step - loss: 1.1862 - val_loss: 1.0645\n",
      "Epoch 8/200\n",
      "40/40 [==============================] - 0s 922us/step - loss: 0.8710 - val_loss: 0.8638\n",
      "Epoch 9/200\n",
      "40/40 [==============================] - 0s 986us/step - loss: 0.7680 - val_loss: 0.7673\n",
      "Epoch 10/200\n",
      "40/40 [==============================] - 0s 956us/step - loss: 0.7201 - val_loss: 0.7276\n",
      "Epoch 11/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.7105 - val_loss: 0.6997\n",
      "Epoch 12/200\n",
      "40/40 [==============================] - 0s 958us/step - loss: 0.6791 - val_loss: 0.6800\n",
      "Epoch 13/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6769 - val_loss: 0.6618\n",
      "Epoch 14/200\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.6686 - val_loss: 0.6686\n",
      "Epoch 15/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6785 - val_loss: 0.6193\n",
      "Epoch 16/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6500 - val_loss: 0.6371\n",
      "Epoch 17/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6459 - val_loss: 0.6018\n",
      "Epoch 18/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6430 - val_loss: 0.5966\n",
      "Epoch 19/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6353 - val_loss: 0.5956\n",
      "Epoch 20/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6283 - val_loss: 0.5784\n",
      "Epoch 21/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6316 - val_loss: 0.5678\n",
      "Epoch 22/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6217 - val_loss: 0.5689\n",
      "Epoch 23/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6175 - val_loss: 0.5633\n",
      "Epoch 24/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6168 - val_loss: 0.5683\n",
      "Epoch 25/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6282 - val_loss: 0.6419\n",
      "Epoch 26/200\n",
      "40/40 [==============================] - 0s 975us/step - loss: 0.6304 - val_loss: 0.5813\n",
      "Epoch 27/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6119 - val_loss: 0.5460\n",
      "Epoch 28/200\n",
      "40/40 [==============================] - 0s 966us/step - loss: 0.5935 - val_loss: 0.5343\n",
      "Epoch 29/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.5944 - val_loss: 0.5686\n",
      "Epoch 30/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.5885 - val_loss: 0.5422\n",
      "Epoch 31/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6034 - val_loss: 0.5177\n",
      "Epoch 32/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.5958 - val_loss: 0.5247\n",
      "Epoch 33/200\n",
      "40/40 [==============================] - 0s 916us/step - loss: 0.5853 - val_loss: 0.5122\n",
      "Epoch 34/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.5877 - val_loss: 0.5071\n",
      "Epoch 35/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.5709 - val_loss: 0.5367\n",
      "Epoch 36/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.5721 - val_loss: 0.4992\n",
      "Epoch 37/200\n",
      "40/40 [==============================] - 0s 999us/step - loss: 0.5659 - val_loss: 0.4940\n",
      "Epoch 38/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.5628 - val_loss: 0.4929\n",
      "Epoch 39/200\n",
      "40/40 [==============================] - 0s 986us/step - loss: 0.5589 - val_loss: 0.4933\n",
      "Epoch 40/200\n",
      "40/40 [==============================] - 0s 917us/step - loss: 0.5683 - val_loss: 0.4842\n",
      "Epoch 41/200\n",
      "40/40 [==============================] - 0s 984us/step - loss: 0.5557 - val_loss: 0.4863\n",
      "Epoch 42/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.5580 - val_loss: 0.4825\n",
      "Epoch 43/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.5520 - val_loss: 0.4768\n",
      "Epoch 44/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.5530 - val_loss: 0.4796\n",
      "Epoch 45/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.5563 - val_loss: 0.4695\n",
      "Epoch 46/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.5498 - val_loss: 0.6115\n",
      "Epoch 47/200\n",
      "40/40 [==============================] - 0s 942us/step - loss: 0.5625 - val_loss: 0.4688\n",
      "Epoch 48/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.5433 - val_loss: 0.4673\n",
      "Epoch 49/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.5379 - val_loss: 0.4661\n",
      "Epoch 50/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.5348 - val_loss: 0.4754\n",
      "Epoch 51/200\n",
      "40/40 [==============================] - 0s 947us/step - loss: 0.5354 - val_loss: 0.4594\n",
      "Epoch 52/200\n",
      "40/40 [==============================] - 0s 909us/step - loss: 0.5352 - val_loss: 0.4514\n",
      "Epoch 53/200\n",
      "40/40 [==============================] - 0s 1000us/step - loss: 0.5280 - val_loss: 0.4634\n",
      "Epoch 54/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.5279 - val_loss: 0.4467\n",
      "Epoch 55/200\n",
      "40/40 [==============================] - 0s 960us/step - loss: 0.5302 - val_loss: 0.4469\n",
      "Epoch 56/200\n",
      "40/40 [==============================] - 0s 934us/step - loss: 0.5225 - val_loss: 0.4578\n",
      "Epoch 57/200\n",
      "40/40 [==============================] - 0s 953us/step - loss: 0.5319 - val_loss: 0.4394\n",
      "Epoch 58/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.5261 - val_loss: 0.4334\n",
      "Epoch 59/200\n",
      "40/40 [==============================] - 0s 952us/step - loss: 0.5152 - val_loss: 0.4565\n",
      "Epoch 60/200\n",
      "40/40 [==============================] - 0s 959us/step - loss: 0.5217 - val_loss: 0.4293\n",
      "Epoch 61/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.5164 - val_loss: 0.4426\n",
      "Epoch 62/200\n",
      "40/40 [==============================] - 0s 995us/step - loss: 0.5101 - val_loss: 0.4343\n",
      "Epoch 63/200\n",
      "40/40 [==============================] - 0s 923us/step - loss: 0.5068 - val_loss: 0.4301\n",
      "Epoch 64/200\n",
      "40/40 [==============================] - 0s 912us/step - loss: 0.5133 - val_loss: 0.4201\n",
      "Epoch 65/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.5030 - val_loss: 0.4207\n",
      "Epoch 66/200\n",
      "40/40 [==============================] - 0s 933us/step - loss: 0.4963 - val_loss: 0.4378\n",
      "Epoch 67/200\n",
      "40/40 [==============================] - 0s 927us/step - loss: 0.4984 - val_loss: 0.4182\n",
      "Epoch 68/200\n",
      "40/40 [==============================] - 0s 984us/step - loss: 0.5021 - val_loss: 0.4135\n",
      "Epoch 69/200\n",
      "40/40 [==============================] - 0s 932us/step - loss: 0.4934 - val_loss: 0.4186\n",
      "Epoch 70/200\n",
      "40/40 [==============================] - 0s 990us/step - loss: 0.5119 - val_loss: 0.4218\n",
      "Epoch 71/200\n",
      "40/40 [==============================] - 0s 939us/step - loss: 0.4900 - val_loss: 0.4158\n",
      "Epoch 72/200\n",
      "40/40 [==============================] - 0s 938us/step - loss: 0.4885 - val_loss: 0.4057\n",
      "Epoch 73/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4866 - val_loss: 0.4124\n",
      "Epoch 74/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4829 - val_loss: 0.4297\n",
      "Epoch 75/200\n",
      "40/40 [==============================] - 0s 993us/step - loss: 0.4854 - val_loss: 0.4078\n",
      "Epoch 76/200\n",
      "40/40 [==============================] - 0s 913us/step - loss: 0.4855 - val_loss: 0.4116\n",
      "Epoch 77/200\n",
      "40/40 [==============================] - 0s 923us/step - loss: 0.4990 - val_loss: 0.3998\n",
      "Epoch 78/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4804 - val_loss: 0.4115\n",
      "Epoch 79/200\n",
      "40/40 [==============================] - 0s 966us/step - loss: 0.4860 - val_loss: 0.3972\n",
      "Epoch 80/200\n",
      "40/40 [==============================] - 0s 989us/step - loss: 0.4795 - val_loss: 0.4070\n",
      "Epoch 81/200\n",
      "40/40 [==============================] - 0s 945us/step - loss: 0.4852 - val_loss: 0.3990\n",
      "Epoch 82/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 936us/step - loss: 0.4786 - val_loss: 0.3977\n",
      "Epoch 83/200\n",
      "40/40 [==============================] - 0s 906us/step - loss: 0.4848 - val_loss: 0.4347\n",
      "Epoch 84/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4900 - val_loss: 0.4163\n",
      "Epoch 85/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4803 - val_loss: 0.4084\n",
      "Epoch 86/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4734 - val_loss: 0.3977\n",
      "Epoch 87/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4763 - val_loss: 0.4487\n",
      "Epoch 88/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4782 - val_loss: 0.3878\n",
      "Epoch 89/200\n",
      "40/40 [==============================] - 0s 975us/step - loss: 0.4771 - val_loss: 0.3889\n",
      "Epoch 90/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4725 - val_loss: 0.4099\n",
      "Epoch 91/200\n",
      "40/40 [==============================] - 0s 962us/step - loss: 0.4852 - val_loss: 0.3899\n",
      "Epoch 92/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4702 - val_loss: 0.3954\n",
      "Epoch 93/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4695 - val_loss: 0.3858\n",
      "Epoch 94/200\n",
      "40/40 [==============================] - 0s 970us/step - loss: 0.4796 - val_loss: 0.3883\n",
      "Epoch 95/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4703 - val_loss: 0.3852\n",
      "Epoch 96/200\n",
      "40/40 [==============================] - 0s 991us/step - loss: 0.4681 - val_loss: 0.3873\n",
      "Epoch 97/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4686 - val_loss: 0.4002\n",
      "Epoch 98/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4872 - val_loss: 0.3865\n",
      "Epoch 99/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4697 - val_loss: 0.3840\n",
      "Epoch 100/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4698 - val_loss: 0.3798\n",
      "Epoch 101/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4788 - val_loss: 0.3819\n",
      "Epoch 102/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4698 - val_loss: 0.3829\n",
      "Epoch 103/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4642 - val_loss: 0.3879\n",
      "Epoch 104/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4654 - val_loss: 0.3801\n",
      "Epoch 105/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4631 - val_loss: 0.3765\n",
      "Epoch 106/200\n",
      "40/40 [==============================] - 0s 983us/step - loss: 0.4630 - val_loss: 0.3985\n",
      "Epoch 107/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4702 - val_loss: 0.3768\n",
      "Epoch 108/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4702 - val_loss: 0.3974\n",
      "Epoch 109/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4677 - val_loss: 0.3740\n",
      "Epoch 110/200\n",
      "40/40 [==============================] - 0s 972us/step - loss: 0.4651 - val_loss: 0.3774\n",
      "Epoch 111/200\n",
      "40/40 [==============================] - 0s 919us/step - loss: 0.4607 - val_loss: 0.3813\n",
      "Epoch 112/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4642 - val_loss: 0.3743\n",
      "Epoch 113/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4608 - val_loss: 0.3828\n",
      "Epoch 114/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4693 - val_loss: 0.3751\n",
      "Epoch 115/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4653 - val_loss: 0.3932\n",
      "Epoch 116/200\n",
      "40/40 [==============================] - 0s 949us/step - loss: 0.4696 - val_loss: 0.4004\n",
      "Epoch 117/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4709 - val_loss: 0.3818\n",
      "Epoch 118/200\n",
      "40/40 [==============================] - 0s 928us/step - loss: 0.4589 - val_loss: 0.3764\n",
      "Epoch 119/200\n",
      "40/40 [==============================] - 0s 971us/step - loss: 0.4729 - val_loss: 0.3859\n",
      "Epoch 120/200\n",
      "40/40 [==============================] - 0s 946us/step - loss: 0.4569 - val_loss: 0.3682\n",
      "Epoch 121/200\n",
      "40/40 [==============================] - 0s 1000us/step - loss: 0.4584 - val_loss: 0.3738\n",
      "Epoch 122/200\n",
      "40/40 [==============================] - 0s 976us/step - loss: 0.4586 - val_loss: 0.3694\n",
      "Epoch 123/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4583 - val_loss: 0.3696\n",
      "Epoch 124/200\n",
      "40/40 [==============================] - 0s 977us/step - loss: 0.4565 - val_loss: 0.3659\n",
      "Epoch 125/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4592 - val_loss: 0.3711\n",
      "Epoch 126/200\n",
      "40/40 [==============================] - 0s 913us/step - loss: 0.4599 - val_loss: 0.3661\n",
      "Epoch 127/200\n",
      "40/40 [==============================] - 0s 991us/step - loss: 0.4564 - val_loss: 0.3811\n",
      "Epoch 128/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4829 - val_loss: 0.4256\n",
      "Epoch 129/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4625 - val_loss: 0.3665\n",
      "Epoch 130/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4514 - val_loss: 0.3705\n",
      "Epoch 131/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4568 - val_loss: 0.3829\n",
      "Epoch 132/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4700 - val_loss: 0.3790\n",
      "Epoch 133/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4549 - val_loss: 0.3700\n",
      "Epoch 134/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4552 - val_loss: 0.3663\n",
      "Epoch 135/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4577 - val_loss: 0.3647\n",
      "Epoch 136/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4619 - val_loss: 0.3865\n",
      "Epoch 137/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4564 - val_loss: 0.3875\n",
      "Epoch 138/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4614 - val_loss: 0.3647\n",
      "Epoch 139/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4510 - val_loss: 0.3623\n",
      "Epoch 140/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4517 - val_loss: 0.3669\n",
      "Epoch 141/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4533 - val_loss: 0.3686\n",
      "Epoch 142/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4586 - val_loss: 0.3628\n",
      "Epoch 143/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4575 - val_loss: 0.3661\n",
      "Epoch 144/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4553 - val_loss: 0.3659\n",
      "Epoch 145/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4637 - val_loss: 0.3753\n",
      "Epoch 146/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4560 - val_loss: 0.3795\n",
      "Epoch 147/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4602 - val_loss: 0.3697\n",
      "Epoch 148/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4643 - val_loss: 0.3862\n",
      "Epoch 149/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4527 - val_loss: 0.3608\n",
      "Epoch 150/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4500 - val_loss: 0.3656\n",
      "Epoch 151/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4500 - val_loss: 0.3673\n",
      "Epoch 152/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4531 - val_loss: 0.3903\n",
      "Epoch 153/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4614 - val_loss: 0.3641\n",
      "Epoch 154/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4510 - val_loss: 0.3950\n",
      "Epoch 155/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4598 - val_loss: 0.3572\n",
      "Epoch 156/200\n",
      "40/40 [==============================] - 0s 907us/step - loss: 0.4554 - val_loss: 0.3818\n",
      "Epoch 157/200\n",
      "40/40 [==============================] - 0s 933us/step - loss: 0.4554 - val_loss: 0.3575\n",
      "Epoch 158/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4494 - val_loss: 0.3584\n",
      "Epoch 159/200\n",
      "40/40 [==============================] - 0s 987us/step - loss: 0.4487 - val_loss: 0.3659\n",
      "Epoch 160/200\n",
      "40/40 [==============================] - 0s 983us/step - loss: 0.4494 - val_loss: 0.3678\n",
      "Epoch 161/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4471 - val_loss: 0.3707\n",
      "Epoch 162/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4503 - val_loss: 0.3573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4498 - val_loss: 0.3635\n",
      "Epoch 164/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4510 - val_loss: 0.3651\n",
      "Epoch 165/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4518 - val_loss: 0.3604\n",
      "Epoch 166/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4521 - val_loss: 0.3739\n",
      "Epoch 167/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4559 - val_loss: 0.3911\n",
      "Epoch 168/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4497 - val_loss: 0.3593\n",
      "Epoch 169/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4506 - val_loss: 0.3574\n",
      "Epoch 170/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4509 - val_loss: 0.3553\n",
      "Epoch 171/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4499 - val_loss: 0.3641\n",
      "Epoch 172/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4513 - val_loss: 0.3614\n",
      "Epoch 173/200\n",
      "40/40 [==============================] - 0s 992us/step - loss: 0.4541 - val_loss: 0.3609\n",
      "Epoch 174/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4514 - val_loss: 0.3549\n",
      "Epoch 175/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4468 - val_loss: 0.3686\n",
      "Epoch 176/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4668 - val_loss: 0.3589\n",
      "Epoch 177/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4470 - val_loss: 0.3636\n",
      "Epoch 178/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4450 - val_loss: 0.3537\n",
      "Epoch 179/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4586 - val_loss: 0.3593\n",
      "Epoch 180/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4775 - val_loss: 0.4038\n",
      "Epoch 181/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4467 - val_loss: 0.3561\n",
      "Epoch 182/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4520 - val_loss: 0.3749\n",
      "Epoch 183/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4465 - val_loss: 0.3557\n",
      "Epoch 184/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4456 - val_loss: 0.3611\n",
      "Epoch 185/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4517 - val_loss: 0.3528\n",
      "Epoch 186/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4430 - val_loss: 0.3676\n",
      "Epoch 187/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4458 - val_loss: 0.3637\n",
      "Epoch 188/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4508 - val_loss: 0.3897\n",
      "Epoch 189/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4591 - val_loss: 0.3651\n",
      "Epoch 190/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4439 - val_loss: 0.3535\n",
      "Epoch 191/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4432 - val_loss: 0.3571\n",
      "Epoch 192/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4450 - val_loss: 0.3507\n",
      "Epoch 193/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4435 - val_loss: 0.3551\n",
      "Epoch 194/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4516 - val_loss: 0.3620\n",
      "Epoch 195/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4466 - val_loss: 0.3686\n",
      "Epoch 196/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4490 - val_loss: 0.3571\n",
      "Epoch 197/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4441 - val_loss: 0.3517\n",
      "Epoch 198/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4515 - val_loss: 0.3637\n",
      "Epoch 199/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4475 - val_loss: 0.3514\n",
      "Epoch 200/200\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4478 - val_loss: 0.3567\n"
     ]
    }
   ],
   "source": [
    "best_hps = tuner.get_best_hyperparameters()[0]\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(train_features, train_labels, validation_data=(val_features, val_labels), epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "normalization (Normalization (None, 11)                23        \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 72        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 21        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 24        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 28        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 30        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 21        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5)                 20        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 257\n",
      "Trainable params: 234\n",
      "Non-trainable params: 23\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='training data')\n",
    "plt.plot(history.history['val_loss'], label='validation data')\n",
    "plt.title('Losses')\n",
    "plt.ylabel('Loss value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 668us/step - loss: 0.4788\n",
      "0.47875499725341797\n",
      "5/5 [==============================] - 0s 742us/step - loss: 0.5645\n",
      "0.5644645094871521\n"
     ]
    }
   ],
   "source": [
    "loss = best_model.evaluate(val_features, val_labels)\n",
    "print(loss)\n",
    "\n",
    "loss = best_model.evaluate(test_features, test_labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.62930393 5.        ]\n",
      " [5.67275095 6.        ]\n",
      " [5.41286707 5.        ]\n",
      " [5.91618013 5.        ]\n",
      " [5.21224022 5.        ]\n",
      " [5.50444603 5.        ]\n",
      " [5.0693779  5.        ]\n",
      " [5.24254227 4.        ]\n",
      " [5.59542513 5.        ]\n",
      " [5.68423605 5.        ]\n",
      " [5.12231112 5.        ]\n",
      " [5.74972439 6.        ]\n",
      " [5.71963358 7.        ]\n",
      " [5.14319611 5.        ]\n",
      " [5.30970097 5.        ]\n",
      " [5.13594103 5.        ]\n",
      " [5.98638248 4.        ]\n",
      " [5.05425453 5.        ]\n",
      " [5.23267508 5.        ]\n",
      " [5.55570412 6.        ]\n",
      " [5.52637672 5.        ]\n",
      " [5.49290514 5.        ]\n",
      " [5.49290514 5.        ]\n",
      " [5.43286133 4.        ]\n",
      " [5.56216383 5.        ]\n",
      " [5.51660347 5.        ]\n",
      " [5.09861946 5.        ]\n",
      " [6.28669405 7.        ]\n",
      " [5.6784811  5.        ]\n",
      " [5.96623182 7.        ]\n",
      " [5.44412756 5.        ]\n",
      " [5.72895575 6.        ]\n",
      " [6.0391264  5.        ]\n",
      " [5.50491858 5.        ]\n",
      " [5.46366262 6.        ]\n",
      " [5.5374403  6.        ]\n",
      " [6.25714445 6.        ]\n",
      " [5.54877567 5.        ]\n",
      " [5.14439917 5.        ]\n",
      " [5.83813667 6.        ]\n",
      " [5.56725645 6.        ]\n",
      " [5.9709568  6.        ]\n",
      " [6.38191414 7.        ]\n",
      " [5.66150522 6.        ]\n",
      " [6.38191414 7.        ]\n",
      " [5.5858984  6.        ]\n",
      " [6.70157766 8.        ]\n",
      " [5.31240702 5.        ]\n",
      " [5.47805071 5.        ]\n",
      " [5.62478447 6.        ]\n",
      " [6.45624065 7.        ]\n",
      " [5.65601826 5.        ]\n",
      " [5.7836504  6.        ]\n",
      " [5.46775818 6.        ]\n",
      " [5.72973251 6.        ]\n",
      " [5.24482584 5.        ]\n",
      " [6.2105093  7.        ]\n",
      " [5.65769339 6.        ]\n",
      " [5.32621861 5.        ]\n",
      " [5.54446697 6.        ]\n",
      " [6.56674862 7.        ]\n",
      " [5.48516607 5.        ]\n",
      " [6.45206594 6.        ]\n",
      " [5.49999094 4.        ]\n",
      " [6.30020905 6.        ]\n",
      " [5.34821463 7.        ]\n",
      " [5.90656328 6.        ]\n",
      " [5.0567255  5.        ]\n",
      " [5.1081934  5.        ]\n",
      " [5.22922897 5.        ]\n",
      " [5.4453063  5.        ]\n",
      " [5.31963015 5.        ]\n",
      " [5.43596458 6.        ]\n",
      " [5.51301432 6.        ]\n",
      " [5.2642374  6.        ]\n",
      " [5.63366032 6.        ]\n",
      " [5.39959908 6.        ]\n",
      " [5.39326859 6.        ]\n",
      " [5.40155029 6.        ]\n",
      " [6.64564943 7.        ]\n",
      " [6.84929228 7.        ]\n",
      " [5.56808281 5.        ]\n",
      " [5.60357952 5.        ]\n",
      " [5.2215662  5.        ]\n",
      " [6.07822371 6.        ]\n",
      " [5.7526989  4.        ]\n",
      " [5.59119511 5.        ]\n",
      " [5.50659227 6.        ]\n",
      " [4.95739174 5.        ]\n",
      " [5.26539993 6.        ]\n",
      " [5.82648468 6.        ]\n",
      " [6.4194169  7.        ]\n",
      " [6.14225483 5.        ]\n",
      " [6.74230051 7.        ]\n",
      " [6.17453575 7.        ]\n",
      " [4.89723825 5.        ]\n",
      " [6.2501483  7.        ]\n",
      " [6.06102228 5.        ]\n",
      " [5.52150774 5.        ]\n",
      " [6.00705147 7.        ]\n",
      " [6.49831581 7.        ]\n",
      " [5.62403059 6.        ]\n",
      " [5.87103033 7.        ]\n",
      " [6.11620426 7.        ]\n",
      " [6.35603333 7.        ]\n",
      " [6.05485964 7.        ]\n",
      " [4.90911341 5.        ]\n",
      " [5.601964   6.        ]\n",
      " [5.37995577 5.        ]\n",
      " [6.51105595 6.        ]\n",
      " [6.42551231 7.        ]\n",
      " [6.16064692 5.        ]\n",
      " [6.43461752 7.        ]\n",
      " [6.06950903 6.        ]\n",
      " [5.82284546 5.        ]\n",
      " [6.39399862 7.        ]\n",
      " [6.66533995 7.        ]\n",
      " [5.80235529 6.        ]\n",
      " [6.48161221 6.        ]\n",
      " [6.00924873 6.        ]\n",
      " [5.99888945 7.        ]\n",
      " [6.27999353 7.        ]\n",
      " [6.27999353 7.        ]\n",
      " [6.30603886 6.        ]\n",
      " [5.335958   6.        ]\n",
      " [5.68946409 5.        ]\n",
      " [5.80062819 6.        ]\n",
      " [5.09786367 6.        ]\n",
      " [5.0588994  4.        ]\n",
      " [5.15943909 5.        ]\n",
      " [6.21696329 6.        ]\n",
      " [5.6573844  6.        ]\n",
      " [5.70180845 6.        ]\n",
      " [5.54707336 5.        ]\n",
      " [5.40230608 5.        ]\n",
      " [6.20146465 6.        ]\n",
      " [5.24113035 5.        ]\n",
      " [5.10026932 6.        ]\n",
      " [6.01591825 6.        ]\n",
      " [5.16290617 5.        ]\n",
      " [5.51260138 5.        ]\n",
      " [5.20433044 6.        ]\n",
      " [5.94602442 5.        ]\n",
      " [6.19164419 7.        ]\n",
      " [5.5171814  6.        ]\n",
      " [5.3046298  5.        ]\n",
      " [6.03857756 5.        ]\n",
      " [6.56557846 7.        ]\n",
      " [5.72762251 7.        ]\n",
      " [6.32360697 6.        ]\n",
      " [5.71572638 5.        ]\n",
      " [5.31078959 5.        ]\n",
      " [5.25354481 5.        ]\n",
      " [5.81480074 6.        ]\n",
      " [5.44661474 6.        ]\n",
      " [5.78405142 6.        ]\n",
      " [5.59538698 6.        ]\n",
      " [5.15577602 5.        ]\n",
      " [6.18829346 6.        ]\n",
      " [6.438972   7.        ]]\n"
     ]
    }
   ],
   "source": [
    "predicted_test_labels = model.predict(test_features)\n",
    "print(np.column_stack([predicted_test_labels, test_labels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
